{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What are the key tasks that machine learning entails? What does data pre-processing imply ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Data: The first step in the Machine Learning process is getting data.\n",
    "\n",
    "Cleaning, Preparing & Manipulating Data: Real-world data often has unorganized, missing, or noisy elements.\n",
    "\n",
    "Train Model: This step is where the magic happens!\n",
    "\n",
    "Testing Model.\n",
    "\n",
    "Improving model.\n",
    "\n",
    "Data preprocessing involves transforming raw data to well-formed data sets so that data mining analytics can be applied.\n",
    "\n",
    "Preprocessing involves both data validation and data imputation\n",
    "\n",
    "The Goal of Data Validation is to assess whether the data in question is both complete and accurate.\n",
    "\n",
    "The Goal of Data Imputation is to correct errors and input missing values, Either Manually or Automatically through business process automation (BPA) programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Describe quantitative and qualitative data in depth. Make a distinction between the two ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantitative Data Type: This Type Of Data Type Consists Of Numerical Values. Anything Which Is Measured By Numbers. E.G., Profit, Quantity Sold, Height, Weight, Temperature, Etc. This Is Again Of Two Types\n",
    "\n",
    "Discrete Data Type: – The Numeric Data Which Have Discrete Values Or Whole Numbers. This Type Of Variable Value If Expressed In Decimal Format Will Have No Proper Meaning. Their Values Can Be Counted. E.G.: – No. Of Cars You Have, No. Of Marbles In Containers, Students In A Class, Etc.\n",
    "Continuous Data Type: – The Numerical Measures Which Can Take The Value Within A Certain Range. This Type Of Variable Value If Expressed In Decimal Format Has True Meaning. Their Values Can Not Be Counted But Measured. The Value Can Be Infinite E.G.: Height, Weight, Time, Area, Distance, Measurement Of Rainfall, Etc.\n",
    "Qualitative Data Type: These Are The Data Types That Cannot Be Expressed In Numbers. This Describes Categories Or Groups And Is Hence Known As The Categorical Data Type. This Can Be Divided Into:-\n",
    "\n",
    "Structured Data: This Type Of Data Is Either Number Or Words. This Can Take Numerical Values But Mathematical Operations Cannot Be Performed On It. This Type Of Data Is Expressed In Tabular Format. E.G.) Sunny=1, Cloudy=2, Windy=3 Or Binary Form Data Like 0 Or1, Good Or Bad, Etc.\n",
    "Unstructured Data: This Type Of Data Does Not Have The Proper Format And Therefore Known As Unstructured Data.This Comprises Textual Data, Sounds, Images, Videos, Etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Create a basic data collection that includes some sample records. Have at least one attribute from each of the machine learning data types ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine What Information You Want to Collect\n",
    "\n",
    "Set a Timeframe for Data Collection\n",
    "\n",
    "Determine Your Data Collection Method\n",
    "\n",
    "Collect the Data\n",
    "\n",
    "Analyze the Data and Implement Your Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What are the various causes of machine learning data issues? What are the ramifications ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noisy data, dirty data, and incomplete data are the quintessential enemies of ideal Machine Learning. The solution to this conundrum is to take the time to evaluate and scope data with meticulous data governance, data integration, and data exploration until you get clear data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Demonstrate various approaches to categorical data exploration with appropriate examples ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Unique value count: One of the first things which can be useful during data exploration is to see how many unique values\n",
    "are there in categorical columns\n",
    "\n",
    "2. Correlation Heat-map between all numeric columns: The term correlation refers to a mutual relationship or association between two things\n",
    "    \n",
    "3. Histogram: Histogram are one of the data scientists favourite data exploration techniques. It gives information on the range of values in which most of the values fall. It also gives information on whether there is any skew in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Describe the various methods for dealing with missing data values in depth ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Delete the observations\n",
    "2.Replace missing values with the most frequent value\n",
    "3.Develop a model to predict missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. What are the various data pre-processing techniques? Explain dimensionality reduction and function selection in a few words ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Various Data Pre-Processing Techniques are:\n",
    "\n",
    "Data Cleaning: The data can have many irrelevant and missing parts. To handle this part, data cleaning is done. It involves handling of missing data, noisy data etc.\n",
    "\n",
    "1.1. Missing Data: This situation arises when some data is missing in the data. It can be handled in various ways. Some of them are: Ignore the tuples: This approach is suitable only when the dataset we have is quite large and multiple values are missing within a tuple. Fill the Missing values: There are various ways to do this task. You can choose to fill the missing values manually, by attribute mean or the most probable value. 1.2.Noisy Data: Noisy data is a meaningless data that can’t be interpreted by machines.It can be generated due to faulty data collection, data entry errors etc. It can be handled in following ways : Binning Method: This method works on sorted data in order to smooth it. The whole data is divided into segments of equal size and then various methods are performed to complete the task. Each segmented is handled separately. One can replace all data in a segment by its mean or boundary values can be used to complete the task. Regression: Here data can be made smooth by fitting it to a regression function.The regression used may be linear (having one independent variable) or multiple (having multiple independent variables). Clustering: This approach groups the similar data in a cluster. The outliers may be undetected or it will fall outside the clusters.\n",
    "\n",
    "1.3.Data Reduction: Since data mining is a technique that is used to handle huge amount of data. While working with huge volume of data, analysis became harder in such cases. In order to get rid of this, we uses data reduction technique. It aims to increase the storage efficiency and reduce data storage and analysis costs. The various steps to data reduction are: Data Cube Aggregation: Aggregation operation is applied to data for the construction of the data cube. Attribute Subset Selection: The highly relevant attributes should be used, rest all can be discarded. For performing attribute selection, one can use level of significance and p- value of the attribute.the attribute having p-value greater than significance level can be discarded. Numerosity Reduction: This enable to store the model of data instead of whole data, for example: Regression Models. Dimensionality Reduction: This reduce the size of data by encoding mechanisms.It can be lossy or lossless. If after reconstruction from compressed data, original data can be retrieved, such reduction are called lossless reduction else it is called lossy reduction. The two effective methods of dimensionality reduction are:Wavelet transforms and PCA (Principal Component Analysis).\n",
    "\n",
    "Feature selection is simply selecting and excluding given features without changing them. Dimensionality reduction transforms features into a lower dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. Make brief notes on any two of the following ?\n",
    "\n",
    "Data collected at regular intervals\n",
    "The gap between the quartiles\n",
    "Use a cross-tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data collected at regular intervals:\n",
    "\n",
    "Interval data is one of the two types of discrete data. An example of interval data is the data collected on a thermometer—its gradation or markings are equidistant. Unlike ordinal data, interval data always take numerical values where the distance between two points on the scale is standardised and equal. The gap between the quartiles:\n",
    "\n",
    "Q1 is the first quartile of the data, i.e., to say 25% of the data lies between minimum and Q1. Q3 is the third quartile of the data, i.e., to say 75% of the data lies between minimum and Q3. The difference between Q3 and Q1 is called the Inter-Quartile Range or IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
