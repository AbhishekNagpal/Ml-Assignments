{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1).Is there any way to combine five different models that have all been trained on the same \n",
    "training data and have all achieved 95 percent precision? If so, how can you go about doing \n",
    "it? If not, what is the reason ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It is possible to combine two or more models but it iwe cant get 95 percent precision as all\n",
    "other models give different precision rate accuracy is differed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2). What is the difference between hard voting classifiers and soft voting classifiers ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " In hard voting (also known as majority voting), every individual classifier votes for a \n",
    "class, and the majority wins. In statistical terms, the predicted target label of the \n",
    "ensemble is the mode of the distribution of individually predicted labels.\n",
    "\n",
    "In soft voting, every individual classifier provides a probability value that a specific data\n",
    "point belongs to a particular target class. The predictions are weighted by the classifiers\n",
    "importance and summed up. Then the target label with the greatest sum of weighted \n",
    "probabilities wins the vote.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3). Is it possible to distribute a bagging ensembles training through several servers to\n",
    "speed up the process? Pasting ensembles, boosting ensembles, Random Forests, and stacking \n",
    "ensembles are all options ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Boosting is a general ensemble method that creates a strong classifier from a number of weak\n",
    "classifiers. This is done by building a model from the training data, then creating a second\n",
    "model that attempts to correct the errors from the first model. It is the best starting point\n",
    "for understanding boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4). What is the advantage of evaluating out of the bag ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It requires less computation and allows us to test the model as being trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5). What distinguishes Extra-Trees from ordinary Random Forests? What good would this extra\n",
    "randomness do? Is it true that Extra-Tree Random Forests are slower or faster than normal \n",
    "Random Forests ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random forest uses bootstrap replicas, that is to say, it subsamples the input data with\n",
    "replacement, whereas Extra Trees use the whole original sample. This may increase variance\n",
    "because bootstrapping makes it more diversified.\n",
    "\n",
    "Random forest adds additional randomness to the model, while growing the trees.\n",
    "Instead of searching for the most important feature while splitting a node, it searches for\n",
    "the best feature among a random subset of features. This results in a wide diversity that \n",
    "generally results in a better model. Extra Trees is much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6). Which hyperparameters and how do you tweak if your AdaBoost ensemble underfits the \n",
    "training data ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If your AdaBoost ensemble underfits the training data, you can try increasing the number of\n",
    "estimators or reducing the regularization hyperparameters of the base estimator. You may also\n",
    "try slightly increasing the learning rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
